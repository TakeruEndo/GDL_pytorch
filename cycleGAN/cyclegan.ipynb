{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7r9qjTJxb6st"
   },
   "source": [
    "## CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 983,
     "status": "ok",
     "timestamp": 1611790956279,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "05904162772435051837"
     },
     "user_tz": -540
    },
    "id": "Ht4YxManb6sx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 969,
     "status": "ok",
     "timestamp": 1611790956280,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "05904162772435051837"
     },
     "user_tz": -540
    },
    "id": "_q0pI8tzb6sz"
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(ResBlock, self).__init__()\n",
    "        block_list = []\n",
    "        self.block = self.make_block(block_list, features)\n",
    "        \n",
    "    def make_block(self, modules_list, features):\n",
    "        modules_list.append(nn.ReflectionPad2d(1))\n",
    "        modules_list.append(nn.Conv2d(features, features, kernel_size=3, stride=1, bias=True))\n",
    "        modules_list.append(self.select_normalization(norm='instance', features=features))\n",
    "        modules_list.append(nn.ReLU(inplace=True))\n",
    "        modules_list.append(nn.ReflectionPad2d(1))\n",
    "        modules_list.append(nn.Conv2d(features, features, kernel_size=3, stride=1, bias=True))\n",
    "        modules_list.append(self.select_normalization(norm='instance', features=features))\n",
    "        modules = nn.Sequential(*modules_list)\n",
    "        return modules\n",
    "        \n",
    "    def select_normalization(self, norm, features):\n",
    "        if norm == 'batch':\n",
    "            return nn.BatchNorm2d(features)\n",
    "        elif norm == 'instance':\n",
    "            return nn.InstanceNorm2d(features)\n",
    "        else:\n",
    "            assert 0, '%s is not supported.' % norm\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.block(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1284,
     "status": "ok",
     "timestamp": 1611790956596,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "05904162772435051837"
     },
     "user_tz": -540
    },
    "id": "myUSGWPzb6s1"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_down, n_up, n_res, in_features):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        out_features = 64\n",
    "        first_conv = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_features, out_features, kernel_size=7, stride=1, padding=0, bias=True),\n",
    "            self.select_normalization(norm='instance', features=out_features),\n",
    "            nn.ReLU(inplace=True)]\n",
    "        \n",
    "        down_block = []\n",
    "        for _ in range(n_down):\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "            down_block += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1, bias=True),\n",
    "                self.select_normalization(norm='instance', features=out_features),\n",
    "                nn.ReLU(inplace=True)]\n",
    "            \n",
    "        res_block = []\n",
    "        res_features = out_features\n",
    "        for _ in range(n_res):\n",
    "            res_block.append(ResBlock(res_features))\n",
    "            \n",
    "        up_block = []\n",
    "        in_features = res_features\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(n_up):\n",
    "            up_block += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True),\n",
    "                self.select_normalization(norm='instance', features=out_features),\n",
    "                nn.ReLU(inplace=True)]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "        \n",
    "        last_conv = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_features, 3, kernel_size=7, stride=1, padding=0, bias=True),\n",
    "            nn.Tanh()]\n",
    "        \n",
    "        self.first_conv = nn.Sequential(*first_conv)\n",
    "        self.down_block = nn.Sequential(*down_block)\n",
    "        self.res_block = nn.Sequential(*res_block)\n",
    "        self.up_block = nn.Sequential(*up_block)\n",
    "        self.last_conv = nn.Sequential(*last_conv)\n",
    "        self.init_weights(self.first_conv)\n",
    "        self.init_weights(self.down_block)\n",
    "        self.init_weights(self.res_block)\n",
    "        self.init_weights(self.up_block)\n",
    "        self.init_weights(self.last_conv)\n",
    "\n",
    "    def init_weights(self, net):\n",
    "        classname = net.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            torch.nn.init.normal_(net.weight.data, 0.0, 0.02)\n",
    "            if hasattr(net, 'bias') and net.bias is not None:\n",
    "                torch.nn.init.constant_(net.bias.data, 0.0)\n",
    "    \n",
    "    def select_normalization(self, norm, features):\n",
    "        if norm == 'batch':\n",
    "            return nn.BatchNorm2d(features)\n",
    "        elif norm == 'instance':\n",
    "            return nn.InstanceNorm2d(features)\n",
    "        else:\n",
    "            assert 0, '%s is not supported.' % norm\n",
    "            \n",
    "    def forward(self, x):\n",
    "        h = self.first_conv(x)\n",
    "        h = self.down_block(h)\n",
    "        h = self.res_block(h)\n",
    "        h = self.up_block(h)\n",
    "        out = self.last_conv(h)   \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1283,
     "status": "ok",
     "timestamp": 1611790956597,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "05904162772435051837"
     },
     "user_tz": -540
    },
    "id": "8M0h0cZQb6s3"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_layers=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        out_features = 64\n",
    "        modules = [nn.Conv2d(3, out_features, kernel_size=4, stride=2, padding=1, bias=True),\n",
    "                   nn.LeakyReLU(negative_slope=0.2, inplace=True)]\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "            if i == n_layers - 1:    stride=1\n",
    "            else:    stride=2\n",
    "            modules += [nn.Conv2d(in_features, out_features, kernel_size=4, stride=stride, padding=1, bias=True),\n",
    "                        self.select_normalization(norm='instance', features=out_features),\n",
    "                        nn.LeakyReLU(negative_slope=0.2, inplace=True)]\n",
    "        \n",
    "        modules += [nn.Conv2d(out_features, 1, kernel_size=4, stride=1, padding=1, bias=True)]\n",
    "        self.layers = nn.Sequential(*modules)\n",
    "        self.init_weights(self.layers)\n",
    "\n",
    "    def init_weights(self, net):\n",
    "        classname = net.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            torch.nn.init.normal_(net.weight.data, 0.0, 0.02)\n",
    "            if hasattr(net, 'bias') and net.bias is not None:\n",
    "                torch.nn.init.constant_(net.bias.data, 0.0)\n",
    "    \n",
    "    def select_normalization(self, norm, features):\n",
    "        if norm == 'batch':\n",
    "            return nn.BatchNorm2d(features)\n",
    "        elif norm == 'instance':\n",
    "            return nn.InstanceNorm2d(features)\n",
    "        else:\n",
    "            assert 0, '%s is not supported.' % norm\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1280,
     "status": "ok",
     "timestamp": 1611790956597,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "05904162772435051837"
     },
     "user_tz": -540
    },
    "id": "42LbNtoTb6s5"
   },
   "outputs": [],
   "source": [
    "class CycleGAN_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datapath, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        # self.A_path = glob(os.path.join('fantasy', '*', '*.jpg'))\n",
    "        # self.B_path = glob(os.path.join('Photos', '*.jpg'))\n",
    "        self.A_path = glob(os.path.join('fantasy2', '*.jpg'))\n",
    "        self.B_path = glob(os.path.join('monet2photo', 'trainB', '*.jpg'))\n",
    "\n",
    "        random.shuffle(self.A_path)\n",
    "        random.shuffle(self.B_path)\n",
    "        self.datalength = min(len(self.A_path), len(self.B_path))\n",
    "        self.dataA = self.A_path[:self.datalength]\n",
    "        self.dataB = self.B_path[:self.datalength]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.datalength\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        imgA = Image.open(self.dataA[i])\n",
    "        imgB = Image.open(self.dataB[i])\n",
    "        \n",
    "        if self.transforms:\n",
    "            imgA = self.transforms(imgA)\n",
    "            imgB = self.transforms(imgB)\n",
    "        \n",
    "        return imgA, imgB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1279,
     "status": "ok",
     "timestamp": 1611790956598,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "05904162772435051837"
     },
     "user_tz": -540
    },
    "id": "EPOcEIJLb6s7"
   },
   "outputs": [],
   "source": [
    "class Image_History_Buffer:\n",
    "    def __init__(self, pool_size=50):\n",
    "        self.pool_size = pool_size\n",
    "        self.buffer = []\n",
    "    \n",
    "    def get_images(self,pre_images):\n",
    "        return_imgs = []\n",
    "        for img in pre_images:\n",
    "            img = torch.unsqueeze(img,0)\n",
    "            if len(self.buffer) < self.pool_size:\n",
    "                self.buffer.append(img)\n",
    "                return_imgs.append(img)\n",
    "            else:\n",
    "                if random.randint(0,1)>0.5:\n",
    "                    i = random.randint(0,self.pool_size-1)\n",
    "                    tmp = self.buffer[i].clone()\n",
    "                    self.buffer[i]=img\n",
    "                    return_imgs.append(tmp)\n",
    "                else:\n",
    "                    return_imgs.append(img)\n",
    "        return torch.cat(return_imgs,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1278,
     "status": "ok",
     "timestamp": 1611790956598,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "05904162772435051837"
     },
     "user_tz": -540
    },
    "id": "laJRoFEWb6s9"
   },
   "outputs": [],
   "source": [
    "class loss_scheduler():\n",
    "    def __init__(self, epoch_decay):\n",
    "        self.epoch_decay = epoch_decay\n",
    "\n",
    "    def f(self, epoch):\n",
    "        if epoch<=self.epoch_decay:\n",
    "            return 1\n",
    "        else:\n",
    "            scaling = 1 - (epoch-self.epoch_decay)/float(self.epoch_decay)\n",
    "            return scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 57246,
     "status": "ok",
     "timestamp": 1611791012568,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "05904162772435051837"
     },
     "user_tz": -540
    },
    "id": "-mpFvV3_b6s_"
   },
   "outputs": [],
   "source": [
    "lr = 0.0002\n",
    "img_size = 256\n",
    "betas = (0.5, 0.999)\n",
    "batchsize = 1\n",
    "imgsize = 256\n",
    "n_epochs = 200\n",
    "decay_epoch = 100\n",
    "lambda_val = 10\n",
    "lambda_id_val = 0\n",
    "datapath = 'apple2orange'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Make training dataset\n",
    "mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
    "transform = transforms.Compose([transforms.Resize(img_size, Image.BICUBIC),\n",
    "                                   transforms.RandomCrop(imgsize),\n",
    "                                   transforms.RandomHorizontalFlip(),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize(mean, std)])\n",
    "train_data = CycleGAN_Dataset(datapath=datapath, transforms=transform)\n",
    "training_dataset = DataLoader(train_data, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "# Define networks\n",
    "G_A2B = Generator(n_down=2, n_up=2, n_res=9, in_features=3).to(device)\n",
    "G_B2A = Generator(n_down=2, n_up=2, n_res=9, in_features=3).to(device)\n",
    "D_A = Discriminator(n_layers=3).to(device)\n",
    "D_B = Discriminator(n_layers=3).to(device)\n",
    "\n",
    "g_opt = optim.Adam(itertools.chain(G_A2B.parameters(), G_B2A.parameters()), lr=lr, betas=betas)\n",
    "d_A_opt = optim.Adam(D_A.parameters(), lr=lr, betas=betas)\n",
    "d_B_opt = optim.Adam(D_B.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "g_lr_scheduler = torch.optim.lr_scheduler.LambdaLR(g_opt, lr_lambda=loss_scheduler(decay_epoch).f)\n",
    "d_a_lr_scheduler = torch.optim.lr_scheduler.LambdaLR(d_A_opt, lr_lambda=loss_scheduler(decay_epoch).f)\n",
    "d_b_lr_scheduler = torch.optim.lr_scheduler.LambdaLR(d_B_opt, lr_lambda=loss_scheduler(decay_epoch).f)\n",
    "\n",
    "adv_loss = nn.MSELoss()\n",
    "l1_norm = nn.L1Loss()\n",
    "criterion_idn = nn.L1Loss()\n",
    "\n",
    "buffer_for_fakeA = Image_History_Buffer()\n",
    "buffer_for_fakeB = Image_History_Buffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 13924988,
     "status": "error",
     "timestamp": 1611822991583,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "05904162772435051837"
     },
     "user_tz": -540
    },
    "id": "6t2r4i8xb6tE",
    "outputId": "f123323b-1ee7-4388-d9af-0995734b785f"
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, n_epochs+1):\n",
    "    G_B2A.train()\n",
    "    G_A2B.train()\n",
    "    D_A.train()\n",
    "    D_B.train()\n",
    "    for idx, (imgA, imgB) in enumerate(training_dataset):\n",
    "        imgA = imgA.to(device)\n",
    "        imgB = imgB.to(device)\n",
    "        imgA_fake, imgB_fake = G_B2A(imgB), G_A2B(imgA)\n",
    "        imgA_rec, imgB_rec = G_B2A(imgB_fake), G_A2B(imgA_fake)\n",
    "        if lambda_id_val > 0:\n",
    "            iden_imgA, iden_imgB = G_B2A(imgA), G_A2B(imgB)\n",
    "        \n",
    "        # Update the discriminator (D_A, D_B)\n",
    "        d_A_opt.zero_grad()\n",
    "        disA_out_real = D_A(imgA)\n",
    "        imgA_fake_ = buffer_for_fakeA.get_images(imgA_fake)\n",
    "        disA_out_fake = D_A(imgA_fake_.detach())\n",
    "        d_lossA_real = adv_loss(disA_out_real, torch.tensor(1.0).expand_as(disA_out_real).to(device))\n",
    "        d_lossA_fake = adv_loss(disA_out_fake, torch.tensor(0.0).expand_as(disA_out_fake).to(device))\n",
    "        disA_loss = (d_lossA_real + d_lossA_fake) * 0.5\n",
    "        disA_loss.backward()\n",
    "        d_A_opt.step()\n",
    "        \n",
    "        d_B_opt.zero_grad()\n",
    "        disB_out_real = D_B(imgB)\n",
    "        imgB_fake_ = buffer_for_fakeB.get_images(imgB_fake)\n",
    "        disB_out_fake = D_B(imgB_fake_.detach())\n",
    "        d_lossB_real = adv_loss(disB_out_real, torch.tensor(1.0).expand_as(disB_out_real).to(device))\n",
    "        d_lossB_fake = adv_loss(disB_out_fake, torch.tensor(0.0).expand_as(disA_out_fake).to(device))\n",
    "        disB_loss = (d_lossB_real + d_lossB_fake) * 0.5\n",
    "        disB_loss.backward()\n",
    "        d_B_opt.step()\n",
    "        \n",
    "        # Update the generator (G)\n",
    "        g_opt.zero_grad()\n",
    "        disB_out_fake = D_B(imgB_fake)\n",
    "        disA_out_fake = D_A(imgA_fake)\n",
    "        g_lossA = adv_loss(disA_out_fake, torch.tensor(1.0).expand_as(disA_out_fake).to(device))\n",
    "        g_lossB = adv_loss(disB_out_fake, torch.tensor(1.0).expand_as(disB_out_fake).to(device))\n",
    "        gen_adv_loss = g_lossA + g_lossB\n",
    "        \n",
    "        cycle_consistency_loss = l1_norm(imgA_rec, imgA) + l1_norm(imgB_rec, imgB)\n",
    "        if lambda_id_val > 0:\n",
    "            identity_loss = criterion_idn(iden_imgA, imgA) + criterion_idn(iden_imgB, imgB)\n",
    "            gen_loss = gen_adv_loss + lambda_val * cycle_consistency_loss + lambda_id_val * identity_loss\n",
    "        else:\n",
    "            gen_loss = gen_adv_loss + lambda_val * cycle_consistency_loss\n",
    "        gen_loss.backward()\n",
    "        g_opt.step()\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print('Training epoch: {} [{}/{} ({:.0f}%)] | D loss (A): {:.6f} | D loss (B): {:.6f} | G loss: {:.6f} | Consistency: {:.6f}  |'\\\n",
    "                  .format(epoch, idx * len(imgA), len(training_dataset.dataset),\n",
    "                  100. * idx / len(training_dataset), disA_loss.item(), disB_loss.item(), gen_loss.item(), cycle_consistency_loss.item()))\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(G_A2B.state_dict(), f'fant_G_A2B_{epoch}.pth')\n",
    "        torch.save(G_B2A.state_dict(), f'fant_G_B2A_{epoch}.pth')\n",
    "        torch.save(D_A.state_dict(), f'fant_D_A_{epoch}.pth')\n",
    "        torch.save(D_B.state_dict(), f'fant_D_B_{epoch}.pth')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUycupIgLODf"
   },
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "aborted",
     "timestamp": 1611822991587,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "05904162772435051837"
     },
     "user_tz": -540
    },
    "id": "O7ik4gPALZpQ"
   },
   "outputs": [],
   "source": [
    "G_A2B.load_state_dict(torch.load('G_A2B_12.pth'))\n",
    "G_B2A.load_state_dict(torch.load('G_B2A_12.pth'))\n",
    "D_A.load_state_dict(torch.load('D_A_12.pth'))\n",
    "D_B.load_state_dict(torch.load('D_B_12.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1611822991587,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "05904162772435051837"
     },
     "user_tz": -540
    },
    "id": "a3i4F1urb6tG"
   },
   "outputs": [],
   "source": [
    "mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
    "crop = transforms.Resize(900)\n",
    "normalize = transforms.Normalize(mean=mean, std=std)\n",
    "to_tensor = transforms.ToTensor()\n",
    "transform = transforms.Compose([crop, to_tensor, normalize])\n",
    "\n",
    "A_path = glob(os.path.join('fantasy', '*', '*.jpg'))[10]\n",
    "B_path = glob(os.path.join('Photos', '*.jpg'))[6]\n",
    "\n",
    "\n",
    "imgA = Image.open(A_path)\n",
    "imgB = Image.open(B_path)\n",
    "imgA_tensor = transform(imgA).to(device)[None,:,:,:]\n",
    "imgB_tensor = transform(imgB).to(device)[None,:,:,:]\n",
    "G_A2B.eval()\n",
    "G_B2A.eval()\n",
    "with torch.no_grad():\n",
    "    fake_B = G_A2B(imgA_tensor)\n",
    "    fake_A = G_B2A(imgB_tensor)\n",
    "    rec_B = G_A2B(fake_A)\n",
    "    rec_A = G_B2A(fake_B)\n",
    "    mean = torch.tensor(mean, dtype=torch.float32)[None,:,None,None].to(device)\n",
    "    std = torch.tensor(std, dtype=torch.float32)[None,:,None,None].to(device)\n",
    "    fake_B = (fake_B * std) + mean\n",
    "    fake_A = (fake_A * std) + mean\n",
    "\n",
    "fake_imgA = Image.fromarray((fake_A * 256.).clamp(min=0, max=255).data.cpu().squeeze().permute(1,2,0).numpy().astype(np.uint8))\n",
    "fake_imgB = Image.fromarray((fake_B * 256.).clamp(min=0, max=255).data.cpu().squeeze().permute(1,2,0).numpy().astype(np.uint8))\n",
    "plt_items = [fake_imgB, fake_imgA]\n",
    "title_list = ['Fake_B', 'Fake_A']\n",
    "rows = 2\n",
    "cols = 1\n",
    "axes=[]\n",
    "fig=plt.figure(figsize=(16, 9))\n",
    "\n",
    "for i in range(rows*cols):\n",
    "    item = plt_items[i]\n",
    "    axes.append( fig.add_subplot(rows, cols, i+1) )\n",
    "    axes[-1].set_title(title_list[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.imshow(item)\n",
    "fig.tight_layout()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1611822991588,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "05904162772435051837"
     },
     "user_tz": -540
    },
    "id": "ws3szHBNoR0V"
   },
   "outputs": [],
   "source": [
    "rec_A = (rec_A * std) + mean\n",
    "Image.fromarray((rec_A * 256.).clamp(min=0, max=255).data.cpu().squeeze().permute(1,2,0).numpy().astype(np.uint8))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CycleGAN_fantazy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}